{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_pred.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CCDTiZkMmP5"
      },
      "source": [
        "!wget -c https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar xvzf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxoGgGtjOgSo"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToNdheXJOnNh"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch.utils.data import random_split, RandomSampler, SequentialSampler \n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import sklearn.metrics\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.model_selection import cross_val_predict"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-HEpsMwrSQ4"
      },
      "source": [
        "Датасет для удобного обращения к данным"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8ufvsiNOpfd"
      },
      "source": [
        "class ReviewsDataset(Dataset):\n",
        "  def __init__(self, root_dir):\n",
        "    self.pos_reviews_dir = root_dir + \"/pos\"\n",
        "    self.pos_reviews_filenames = os.listdir(self.pos_reviews_dir)\n",
        "    self.neg_reviews_dir = root_dir + \"/neg\"\n",
        "    self.neg_reviews_filenames = os.listdir(self.neg_reviews_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.pos_reviews_filenames) + len(self.neg_reviews_filenames)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    sample = {}\n",
        "    if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "    if idx % 2 == 0:\n",
        "      k = idx // 2\n",
        "      review_idx_filename = self.pos_reviews_filenames[k]        \n",
        "      id_and_rating = re.findall(r\"\\d+\", review_idx_filename)\n",
        "      with open(os.path.join(self.pos_reviews_dir, review_idx_filename), \"r\") as review:\n",
        "        data = review.readlines()\n",
        "    else:\n",
        "      k = (idx - 1) // 2\n",
        "      review_idx_filename = self.neg_reviews_filenames[k]\n",
        "      id_and_rating = re.findall(r\"\\d+\", review_idx_filename)\n",
        "      with open(os.path.join(self.neg_reviews_dir, review_idx_filename), \"r\") as review:\n",
        "        data = review.readlines()     \n",
        "    rating = int(id_and_rating[1])\n",
        "    data = str(data)\n",
        "    sample = {\"Text\": data, \"Rating\": rating}\n",
        "    return sample"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVgF4u1TOrMK"
      },
      "source": [
        "data_train = ReviewsDataset(\"/content/aclImdb/train\")\n",
        "data_test = ReviewsDataset(\"/content/aclImdb/test\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVo6QcI3OtLL"
      },
      "source": [
        "reviews_texts_train = []\n",
        "reviews_rating_train = []\n",
        "\n",
        "reviews_texts_test = []\n",
        "reviews_rating_test = []\n",
        "\n",
        "for i in range(len(data_train)):\n",
        "  reviews_texts_train.append(data_train[i]['Text'])\n",
        "  reviews_rating_train.append(data_train[i]['Rating'])\n",
        "  reviews_texts_test.append(data_test[i]['Text'])\n",
        "  reviews_rating_test.append(data_test[i]['Rating'])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1ShPdEXPZ7H"
      },
      "source": [
        "reviews_rating_train = [[1] if rating >= 7 else [0] for rating in reviews_rating_train]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFEgZPbHyoz0"
      },
      "source": [
        "Загрузка предтренированного токенизатора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAIJnk2vO2NZ"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb1Qx7jBI6Q8"
      },
      "source": [
        "Предобработка текстов из тренировочной выборки для BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWMXcQD9PCNw"
      },
      "source": [
        "'''\n",
        "Тексты токенизируются\n",
        "Добавляются токены [CLS] и [SEP]\n",
        "Каждый текст ограничивается 500-ми токенами\n",
        "Формируется attention-маска\n",
        "'''\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in reviews_texts_train:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 500,      \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',    \n",
        "                   )\n",
        "       \n",
        "    input_ids.append(encoded_dict['input_ids']) \n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(reviews_rating_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUGWrff9OFtV"
      },
      "source": [
        "Разделение данных на тренировочный и валидационный датасеты в соотношении 90 к 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8avYgSoPULbs"
      },
      "source": [
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbQoS9YrOn2p"
      },
      "source": [
        "Создание даталоадеров для тренировочного и валидационного датасетов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkufHBqRUdre"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), #Для train используется случайный семплер\n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, \n",
        "            sampler = SequentialSampler(val_dataset), #Для val используется последовательный семплер\n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pjg_A48PDE6"
      },
      "source": [
        "В качетстве вычислительного устройства выбирается ГПУ, если такого нет, то выбирается ЦПУ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "734mpmbAXj_1"
      },
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyXg178tQ_cC"
      },
      "source": [
        "Загрузка предтренированного BERTа для классификации входных последовательностей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBcD5FSoU7NM"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", \n",
        "    num_labels = 2, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False,\n",
        ")\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpYT801TWn3r"
      },
      "source": [
        "Настройка количества эпох тренировки BERTа, оптимизатора и планировщика "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhIzxHraVKmC"
      },
      "source": [
        "epochs = 2\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5,\n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAxrgfk7myYA"
      },
      "source": [
        "Функция для красивого вывода времени"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEvvj9vSVRwR"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD-KaeUmnWRL"
      },
      "source": [
        "Цикл тренировки модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQEzhCGCVUxi"
      },
      "source": [
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    correct_samples = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                       token_type_ids=None, \n",
        "                       attention_mask=b_input_mask, \n",
        "                       labels=b_labels,\n",
        "                       return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "        \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           labels=b_labels,\n",
        "                           return_dict=True)\n",
        "        \n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "        total_eval_loss += loss.item()\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_pred = np.max(logits, axis=1)\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        correct_samples += (label_pred == label_ids).float().sum()\n",
        "        \n",
        "    accuracy = correct_samples / len(val_dataset)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(accuracy))\n",
        "\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij7ZDm4Xng-n"
      },
      "source": [
        "Сохранение модели и токенизатора"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLAhmavWzbZX"
      },
      "source": [
        "output_dir = 'content/model_save/'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  \n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnIQUcWK0Ndi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!cp -r content/model_save/ \"/content/drive/MyDrive/BERT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDHzQVkMoFd-"
      },
      "source": [
        "Преобразование оценок тестовых данных в сентименты"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY4YEAalgTV5"
      },
      "source": [
        "reviews_rating_test = [[1] if rating >= 7 else [0] for rating in reviews_rating_test]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heoG8RxHoJ7E"
      },
      "source": [
        "Предобработка тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGAtGIFzgC_3"
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in reviews_texts_test:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 500,           \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt',    \n",
        "                   )\n",
        "       \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(reviews_rating_test)\n",
        " \n",
        "batch_size = 8  \n",
        "\n",
        "test_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCDctdLcoc0s"
      },
      "source": [
        "Проверка работы модели на тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-SSQU_jg2RO"
      },
      "source": [
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "correct_samples = 0\n",
        "\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "  \n",
        "  if step % 40 == 0 and not step == 0:\n",
        "    print('  Batch {:>5,}  of  {:>5,}..'.format(step, len(test_dataloader)))\n",
        "\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  label_pred = np.max(logits, axis=1)\n",
        "  correct_samples += (label_pred == label_ids).sum()\n",
        "\n",
        "accuracy = correct_samples / len(test_dataset)\n",
        "print(\"  Accuracy: {0:.2f}\".format(accuracy))\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaO9oguDscHo"
      },
      "source": [
        "Функция для получение эмбеддинга отзыва"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoGTyyuosM0q"
      },
      "source": [
        "def get_BERT_embedding(text):\n",
        "    model.eval()\n",
        "    b_input_ids = []\n",
        "    b_input_mask = []\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 500,           \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',     \n",
        "                   )\n",
        "    b_input_ids.append(encoded_dict['input_ids'])\n",
        "    b_input_mask.append(encoded_dict['attention_mask'])\n",
        "    b_input_ids = torch.cat(b_input_ids).to(device)\n",
        "    b_input_mask = torch.cat(b_input_mask).to(device)\n",
        "    with torch.no_grad():\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           return_dict=True)\n",
        "    hidden_states = result.hidden_states\n",
        "    last_hidden_state = hidden_states[-1]\n",
        "    CLS_token = last_hidden_state[:, 0, :]\n",
        "    return CLS_token"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd-9klYUuDgP"
      },
      "source": [
        "Получение эмбеддингов тренировочных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi1vlQ2EuCoV"
      },
      "source": [
        "embedded_texts_train = []\n",
        "i = 0\n",
        "\n",
        "for text in reviews_texts_train:\n",
        "    X = get_BERT_embedding(text)\n",
        "    X_cpu = X.to('cpu')\n",
        "    embedded_texts_train.append(X_cpu)\n",
        "    i += 1\n",
        "    print(i)\n",
        "    \n",
        "tensor_embedded_texts_train = torch.empty((25000, 768))\n",
        "tensor_embedded_ratings_train = torch.empty((25000,))\n",
        "\n",
        "for i, text in enumerate(embedded_texts_train):\n",
        "    tensor_embedded_texts_train[i] = text  \n",
        "    tensor_embedded_ratings_train[i] = pointed_rating_train[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfT_Uj1TuOVP"
      },
      "source": [
        "Получение эмбеддингов тестовых данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "950VZkkOuRYo"
      },
      "source": [
        "embedded_texts_test = []\n",
        "i = 0\n",
        "\n",
        "for text in reviews_texts_test:\n",
        "    X = get_BERT_embedding(text)\n",
        "    X_cpu = X.to('cpu')\n",
        "    embedded_texts_test.append(X_cpu)\n",
        "    i += 1\n",
        "    print(i)\n",
        "    \n",
        "tensor_embedded_texts_test = torch.empty((25000, 768))\n",
        "tensor_embedded_ratings_test = torch.empty((25000,))\n",
        "\n",
        "for i, text in enumerate(embedded_texts_test):\n",
        "    tensor_embedded_texts_test[i] = text\n",
        "    tensor_embedded_ratings_test[i] = pointed_rating_test[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkgbBXBfuXCI"
      },
      "source": [
        "Создаение и фиттинг модели линейной регрессии из sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLP1k3K8uWgF"
      },
      "source": [
        "LR = LinearRegression()\n",
        "LR.fit(tensor_embedded_texts_train_prime, tensor_embedded_ratings_train)\n",
        "print(\"R2 mitrics: \".format(LR.score(tensor_embedded_texts_test, tensor_embedded_ratings_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZGcy324x8_V"
      },
      "source": [
        "Функции софтмакса, классификатора сентимента и выставления оценки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hE-vfGVHxtlz"
      },
      "source": [
        "def softmaxes(prediction):\n",
        "    shifted_pred = prediction - np.max(prediction)\n",
        "    exp_pred = np.exp(shifted_pred)\n",
        "    softmax = exp_pred / np.sum(exp_pred)\n",
        "    return softmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx7L3ZWgxwfz"
      },
      "source": [
        "def predict_sentiment(probs):\n",
        "    sentiment = np.argmax(probs)\n",
        "    if sentiment == 1:\n",
        "        sentiment = \"positive\"\n",
        "    elif sentiment == 0:\n",
        "        sentiment = \"negative\"\n",
        "    return sentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKcOGDJYxyka"
      },
      "source": [
        "def predict_rating(CLS_token, LR_model):\n",
        "    rating = LR_model.predict(CLS_token)\n",
        "    if rating > 10:\n",
        "        rating = 10\n",
        "    elif rating < 1:\n",
        "        rating = 1\n",
        "    else:\n",
        "        rating = round(rating)\n",
        "    return rating"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ValzELshxx90"
      },
      "source": [
        "Функция для выставления рейтинга и сентимента отзыва"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nm8-abVx1Jp"
      },
      "source": [
        "def predict_rating_and_sentiment_with_BERT(text):\n",
        "    model.eval()\n",
        "    b_input_ids = []\n",
        "    b_input_mask = []\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = 500,           \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   \n",
        "                        return_tensors = 'pt',    \n",
        "                   )\n",
        "    b_input_ids.append(encoded_dict['input_ids'])\n",
        "    b_input_mask.append(encoded_dict['attention_mask'])\n",
        "    b_input_ids = torch.cat(b_input_ids).to(device)\n",
        "    b_input_mask = torch.cat(b_input_mask).to(device)\n",
        "    with torch.no_grad():\n",
        "            result = model(b_input_ids, \n",
        "                           token_type_ids=None, \n",
        "                           attention_mask=b_input_mask,\n",
        "                           return_dict=True)\n",
        "    logits = result.logits\n",
        "    logits = logits.detach().cpu().numpy()[0]\n",
        "    probs = softmaxes(logits)\n",
        "    \n",
        "    hidden_states = result.hidden_states\n",
        "    last_hidden_state = hidden_states[-1]\n",
        "    CLS_token = last_hidden_state[:, 0, :]\n",
        "\n",
        "    sentiment = predict_sentiment(probs)\n",
        "    rating = predict_rating(CLS_token)\n",
        "    \n",
        "    return rating, sentiment"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}